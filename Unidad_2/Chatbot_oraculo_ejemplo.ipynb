{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqu/gXrr7TD1OKCopLZLf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vshalisko/PythonIntro/blob/main/Unidad_2/Chatbot_oraculo_ejemplo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will take exactly 4 months, 3 weeks, 2 days, 1 hour, 46 minutes and 15 seconds to learn everything you need to know to build a simple chatbot. (usuario de Reddit)\n",
        "\n",
        "https://www.reddit.com/r/learnpython/comments/1dc80lz/making_simple_chatbot/"
      ],
      "metadata": {
        "id": "odXzBYuNuISA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chatbot inspirado en código disponible en:\n",
        "\n",
        "https://planetachatbot.com/construyendo-chatbot-simple-desde-cero-en-python-usando-nltk/\n",
        "\n"
      ],
      "metadata": {
        "id": "y3DFJx5BuO5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar la biblioteca gensim en el entorno virtual de Python.\n",
        "\n",
        "Nota: Se requiere permitir reiniciar la sesiión de Colab."
      ],
      "metadata": {
        "id": "ey594OBdB_SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install gensim"
      ],
      "metadata": {
        "id": "mOzOer-n4T-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "356e5635-dedc-4a5f-ca02-129fe8073ab9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d5322690e57447feac8e70b3e03eab68"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los componentes (bibliotecas)"
      ],
      "metadata": {
        "id": "5rEu6rvnInAm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qYc7JdqruAfl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "#import random\n",
        "\n",
        "#import string\n",
        "\n",
        "#import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar bases de datos de nltk"
      ],
      "metadata": {
        "id": "Q-CmBv5OIxIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6RgHEQmrbuS",
        "outputId": "5cdf4229-102b-42fc-c201-61a9199c33c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar las palabras de español en la lista de 'stopwprds'"
      ],
      "metadata": {
        "id": "QbEeHFdHJ30O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_sw = set(stopwords.words('spanish'))\n",
        "#print(spanish_sw)"
      ],
      "metadata": {
        "id": "iEDWqGn-zsW2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectar Google Drive del usuario"
      ],
      "metadata": {
        "id": "AJKIGpOuIzSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u0_fpNpmuldh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5532befe-f3fd-48eb-c4f4-d09699ff7b9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leer el archivo ocn base de datos de frases celebres"
      ],
      "metadata": {
        "id": "8VqicodWmHLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Data/'\n",
        "file = 'frases_celebres_seleccion.txt'\n",
        "\n",
        "raw = []\n",
        "with open(path + file, 'r', errors = 'ignore') as f:\n",
        "  raw = f.read()\n",
        "  #print(raw)"
      ],
      "metadata": {
        "id": "_fmwLjIeusfO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convertir texto plano a lista de frases\n",
        "onlysent_tokens = nltk.sent_tokenize(raw)\n",
        "# reemplazar simbolos de nueva linea ocn espacios\n",
        "onlysent_tokens = [sentence.replace('\\n', ' ') for sentence in onlysent_tokens]\n",
        "\n",
        "# generar una copia de texto con en lowercase\n",
        "raw_lower = raw.lower()\n",
        "# convertir texto a lista de palabras\n",
        "word_tokens = nltk.word_tokenize(raw_lower)\n",
        "\n",
        "print(\"\\nNúmero de frases célebres en la base de datos:\")\n",
        "print(len(onlysent_tokens))\n",
        "\n",
        "print(\"\\nEjemplo de frases celebres:\")\n",
        "print(onlysent_tokens[0:3])\n",
        "\n",
        "print(\"\\nNúmero de palabras en labase de datos de frases célebres:\")\n",
        "print(len(word_tokens))\n"
      ],
      "metadata": {
        "id": "RXyVtyVruWRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c0f6bc-cdb2-497d-df76-b8bf0b287bc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número de frases célebres en la base de datos:\n",
            "639\n",
            "\n",
            "Ejemplo de frases celebres:\n",
            "['¿Encontraría a la Maga?', 'Tan de vez en cuando uno tiene ganas de encontrarse a sí mismo.', 'A buen hambre, no hay pan duro.']\n",
            "\n",
            "Número de palabras en labase de datos de frases célebres:\n",
            "7329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construir el modelo Word2Vec a partir de la base de datos de frases célebres"
      ],
      "metadata": {
        "id": "Ms8cOVP8KEqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesar texto para generar el modelo Word2Vec\n",
        "# quitar los simbolos de punctuación y convertir palabras al registro bajo\n",
        "corpus_sentences = [re.sub(r'[^\\w\\s]', '', sentence.lower()).split() for sentence in onlysent_tokens]\n",
        "print(type(corpus_sentences))\n",
        "print(len(corpus_sentences))\n",
        "print(corpus_sentences[0:3])\n",
        "\n",
        "# funcion para qliminar stopwords\n",
        "def remove_stopwords(sentence_words, sw):\n",
        "  filtered_words = [word for word in sentence_words if word not in sw]\n",
        "  return filtered_words\n",
        "\n",
        "# eliminar stopwords de la base de datos\n",
        "filtered_sentences = []\n",
        "for sentence in corpus_sentences:\n",
        "  filtered_sentence = remove_stopwords(sentence, spanish_sw)\n",
        "  filtered_sentences.append(filtered_sentence)\n",
        "\n",
        "print(type(filtered_sentences))\n",
        "print(len(filtered_sentences))\n",
        "print(filtered_sentences[0:3])\n",
        "\n",
        "# Entrenar el modelo Word2Vec\n",
        "model = Word2Vec(min_count=1,\n",
        "                     window=6,\n",
        "                     vector_size=300,\n",
        "                     sample=6e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=4)\n",
        "\n",
        "model.build_vocab(filtered_sentences, progress_per=1000)\n",
        "model.train(filtered_sentences, total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "# Entrenar el modelo Word2Vec (simple)\n",
        "#model = Word2Vec(sentences=filtered_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"\\nCaracteristicas del modelo Word2Vec\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwHAbRfc1Irh",
        "outputId": "fd9050d9-73ba-4974-bd5d-f3a568a09da7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "639\n",
            "[['encontraría', 'a', 'la', 'maga'], ['tan', 'de', 'vez', 'en', 'cuando', 'uno', 'tiene', 'ganas', 'de', 'encontrarse', 'a', 'sí', 'mismo'], ['a', 'buen', 'hambre', 'no', 'hay', 'pan', 'duro']]\n",
            "<class 'list'>\n",
            "639\n",
            "[['encontraría', 'maga'], ['tan', 'vez', 'ganas', 'encontrarse', 'mismo'], ['buen', 'hambre', 'pan', 'duro']]\n",
            "\n",
            "Caracteristicas del modelo Word2Vec\n",
            "Word2Vec<vocab=1534, vector_size=300, alpha=0.03>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preguntar al usuario por una frase"
      ],
      "metadata": {
        "id": "EXgfCo-tKlPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = str(input(\"Introduce su frase: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKTY7I100rLt",
        "outputId": "8ba52894-3053-4cf6-9f8e-228964602eed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Introduce su frase: Un hombre siempre busca la verdad, no se puede decir lo mismo sobre un caracol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparar la frase con la base de datos de frases celebres y buscar la frase celebre con el significado mas cercana a la frase del usuario"
      ],
      "metadata": {
        "id": "LIryo4nGKxNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Funcion para construir el vector de una frase\n",
        "def get_sentence_vector(sentence, model):\n",
        "    # separar frase en palabras\n",
        "    words = sentence.split()\n",
        "\n",
        "    # quitar simbolos de puntuacion\n",
        "    words = re.sub(r'[^\\w\\s]', '', sentence.lower()).split()\n",
        "\n",
        "    # determinar el vector de cada palabra en la frase en forma de lista\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "\n",
        "    # en caso que vector es vacio generar el vector de ceros y regresarlo\n",
        "    if not word_vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # en caso que vector no es vacio generar un promedio y regresarlo\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Get user input vector\n",
        "filtered_user_input = remove_stopwords(user_input.split(), spanish_sw)\n",
        "filtered_user_input_ok = \" \".join(filtered_user_input)\n",
        "user_input_vector = get_sentence_vector(filtered_user_input_ok, model)\n",
        "\n",
        "print(\"Pregunta del ususrio: \" + str(user_input))\n",
        "#print(\"Pregunta del ususrio filtrada: \" + str(filtered_user_input))\n",
        "#print(\"Vector del modelo:\")\n",
        "#print(user_input_vector)\n",
        "\n",
        "# Find the most similar sentence in the corpus\n",
        "most_similar_sentence = \"\"\n",
        "max_similarity = -1\n",
        "\n",
        "for i, sentence in enumerate(onlysent_tokens):\n",
        "    sentence_vector = get_sentence_vector(sentence, model)\n",
        "    if np.linalg.norm(user_input_vector) != 0 and np.linalg.norm(sentence_vector) != 0:\n",
        "        similarity = cosine_similarity([user_input_vector], [sentence_vector])[0][0]\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            most_similar_sentence = onlysent_tokens[i]\n",
        "\n",
        "# La frase de mayor similitud:\n",
        "#print(\"Primera frase\")\n",
        "#print(most_similar_sentence)\n",
        "#print(\"Segunda frase\")\n",
        "#print(most_similar_sentence_2)\n",
        "\n",
        "\n",
        "if not most_similar_sentence:\n",
        "  print(\"No tenogo nada que responder en esta ocasión\")\n",
        "else:\n",
        "  print(\"La respuesta es: \" + most_similar_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5cVy7YJkmYX",
        "outputId": "7b21a0dc-5e5d-4295-dce2-95c348a485de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta del ususrio: Un hombre siempre busca la verdad, no se puede decir lo mismo sobre un caracol\n",
            "La respuesta es: El ser humano es aquel que no puede ser usado meramente como un medio, sino que debe ser siempre considerado al mismo tiempo como un fin.\n"
          ]
        }
      ]
    }
  ]
}